Good Morning,
Note that scala and spark programming test is scheduled on 24th June 2024 form 8.45 am to 10.45 am. You need to execute 1 scala ad 1 spark program. The sample programs are given below for your reference. 
 

Scala Programs

1.      

Write the scala code to implement bubble sort algorithm.

2.      

Write scala code to find the length of each word from the array.

3.      

Write scala code to find the number of books published by each author, referring to the collection given below, with (authorName, BookName)

{ (‘  Dr. Seuss’: ‘How the Grinch Stole Christmas!’) ,(‘ Jon Stone’: ‘Monsters at the End of This Book’ ) , (‘ Dr. Seuss’: ‘The Lorax’ ) ,( ‘Jon Stone’: ‘Big Bird in China’  ) ( ‘ Dr. Seuss’ :’ One Fish, Two Fish, Red Fish, Blue Fish’  ) }

4.      

Write the program to illustrate the use of pattern matching in scala, for the following
Matching on case classes.

Define two case classes as below:
abstract class Notification
case class Email(sender: String, title: String, body: String) extends Notification
case class SMS(caller: String, message: String) extends Notification
Define a function showNotification which takes as a parameter the abstract type Notification and matches on the type of Notification (i.e. it figures out whether it’s an Email or SMS).
In the case it’s an Email(email, title, _) return the string: s"You got an email from $email with title: $title“
In the case it’s an SMS return the String: s"You got an SMS from $number! Message: $message“

5.      

Write the scala program using imperative style to implement quick sort algorithm.

6.      

Write a scala function to convert the each word to capitalize each word in the given sentence.

7.      

Write scala code to show functional style program to implement quick sort algorithm.

8.      

For the below given collection of items with item-names and quantity, write the scala code for the given statement.

Items = {(“Pen”:20), (“Pencil”:10), (“Erasor”:7), (“Book”:25), (“Sheet”:15)}

i.           Display item-name and quantity

ii.          Display sum of quantity and total number of   items

iii.       Add 3 Books to the collection

Add new item “Board” with quantity 15 to the collection

9.      

Develop a scala code to search an element in the given list of numbers. The function Search() will take two arguments: list of numbers and the number to be searched. The function will write True if the number is found, False otherwise.

10.   

Illustrate the implementation by writing scala code to generate a down counter from 10 to 1.

11.   

Design a scala function to perform factorial item in the given collection. The arguments passed to the function are the collection of items. Assume the type of the argument for the function suitably. The return type is to be integer.

12.   

For the below given collection of items with item names and quantity, write the scala code for the given statement. Items = {(“Butter”:20), (“Bun”:10), (“Egg”:7), (“Biscuit”:25),(“Bread”:15)}

i.               Display item-name and quantity

ii.               Display sum of quantity and total number of items

iii.            Add 3 Buns to the collection

      iv. Add new item “Cheese” with quantity 12 to the collection

13.   

Implement function for binary search using recursion in Scala to find the number, given a list of numbers. The function will have two arguments: Sorted list of numbers and the number to be searched.

14.   

Write a function to find the length of each word and return the word with highest length .

Ex for the collection of words = (“games”, “television”,”rope”,”table”)

The function should return (“television”,10). The word with the highest length .

Read the words from the keyboard.

 

Spark Programs

15.   

Analyze the application of fold() and aggregate() functions in Spark by considering a scenario where all the items in a collection are updated by a count of 100. Evaluate the efficiency, performance, and suitability of both.

16.   

Consider a text file text.txt. Develop Spark code to read the file and count the number of occurrences of each word using Spark RDD. Store the result in a file. Display the words which appear more than 4 times.

17.   

Consider the content of text file text.txt. Perform the counting of occurrences of each word using pair RDD.

18.   

Write the Spark Code to print the top 10 tweeters.

Tweet Mining: A dataset with the 8198 reduced tweets, reduced-tweets.json will be provided. The data contains reduced tweets as in the sample below:
{"id":"572692378957430785",
"user":"Srkian_nishu smile",
"text":"@always_nidhi @YouTube no idnt understand bti loved of this mve is rocking",
"place":"Orissa",
"country":"India"}
A function to parse the tweets into an RDD will be provided.

19.   

Simulate the following scenario using Spark streaming. There will be a process which will be streaming lines of text to a unix port using socket communication. The process we can use for this purpose is netcat. It will stream lines typed on the console to a unix socket. The spark application needs to read the lines from the specified port, and it needs to produce the word counts on the console. A batch interval of 5 second can be used.

20.   

Develop the spark code to find the average of marks using the combineByKey() operation.

Sample Input format: Array( ("Joe", "Maths", 83), ("Joe", "Physics", 74), ("Joe", "Chemistry", 91), ("Joe", "Biology", 82), ("Nik", "Maths", 69), ("Nik ", "Physics", 62), ("Nik ", "Chemistry", 97), ("Nik ", "Biology", 80))

21.   

Consider the Employee table with the fields as (EmpID, Dept, EmpDesg). Design the Spark program to partition the table using Dept and construct the hashed partition of 4.

22.   

Consider a collection of 100 items of type integer given in the csv file. Write the Spark code to find the average of these 100 items.

23.   

Consider a collection with items as (11,34,45,67,3,4,90).

      i.         Illustrate how spark context will construct the RDD from the collection, assuming number of partitions to be made is 3.

     ii.         Using mapPartitionsWithIndex return content of each partition along with partition index and apply a function, that increments the value of each element by 1, and returns an array.

24.   

Consider the Item object.

Item = Map{(“Ball”:10), (“Ribbon”:50), (“Box”:20), (“Pen”:5), (“Book”:8), (“Dairy”:4),(“Pin”:20)}

Design the spark program to perform the following

i.               Find how many partitions are created for the collection Item?

ii.              Display the content of the RDD Display the content of each partition separately

25.   

The table 1b provides the distributed data of the scala object

Item = Map{(“Ball”:10), (“Ribbon”:50), (“Box”:20), (“Pen”:5), (“Book”:8), (“Dairy”:4),(“Pin”:20)}

 

Partition1

{(“Ball”:10) (“Ribbon”:50) (“Box”:20)

Partition2

 (“Pen”:5) (“Book”:8)

Partition3

(“Dairy”:4) (“Pin”:20)

 

 

 

Table 1b: Data distribution

Design the spark program to perform the following

i.               Find how many partitions are created for the collection Item?

ii.              Display the content of the RDD

iii.            Display the content of each partition separately.

26.   

Consider the text file words.txt as shown in the figure 1a.

Write the spark code to perform the following.

i)               Count the number of occurrences of each word.

ii)             Arrange the word count in ascending order based on Key.

iii)           Display the words that begin with ‘s’.

She sells sea shells by the seashore

And the shells she sells by the seashore

Are sea shells for sure 

Figure 1. a

27.   

Illustrate the application of combineByKey to combine all the values of the same key in the following collection.

((“coffee”,2),(“cappuccino”,5),(“tea”,3),(“coffee”,10),(“ cappuccino”,15))